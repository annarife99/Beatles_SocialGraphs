{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a0cfae",
   "metadata": {},
   "source": [
    "_**Social Graphs and Interactions** - Project Assignment B  | December 2021_\n",
    "\n",
    "Lluis Colomer - Anna Rif√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ab311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the necessary packages we used for this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "import ast\n",
    "import operator\n",
    "import powerlaw\n",
    "from fa2 import ForceAtlas2\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import urllib.request as urllib2\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.text import TextCollection\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import community.community_louvain\n",
    "import community.community_louvain\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "import spotipy.oauth2 as oauth2\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "import chart_studio\n",
    "import chart_studio.tools as tls\n",
    "import chart_studio.plotly as py\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b5467",
   "metadata": {},
   "source": [
    "# <font color='darkgreen'> <center> <h3>The Network of Beatles Songs</h3> </center>\n",
    "# <font color='green'> <center> <h4>Getting into Beatles' mind</h4> </center>\n",
    "  \n",
    "![Image](https://github.com/annarife99/Beatles_SocialGraphs/blob/main/Images/Beatles.jpg?raw=true)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8479c",
   "metadata": {},
   "source": [
    "**John Lennon**, **Paul McCartney**, **George Harrison** and **Ringo Starr**: The best-known members of the one of the greateast and most influential bands of all times: **The Beatles.** \n",
    "\n",
    "Smart, Idealistic, Playful, Irreverent, Eclectic, among many others. Their youth personification and unmatched innovation led their music, from the not-so-simple love songs they started with to their later perfectionistic studio extravaganzas. \n",
    "\n",
    "Led by primary songwriters Lennon and McCartney, the Beatles evolved from Lennon's previous group, and built their reputation playing clubs in Liverpool and Hamburg over three years from 1960. The core trio of Lennon, McCartney and Harrison, together since 1958, went through a succession of drummers, until Ringo Starr joined them in 1962. \n",
    "\n",
    "In the following years, the Beatles had an accomplished career, but mainly divided into three main periods of relatively similar productivity: the first stage started from the establishment of the band until their attaining worldwide popularity in 1964, the second was from 1965 to 1967 with the most creative and significant output of the group, and the final stage (from 1968 to 1970) where they were working more individually and the band finally folded in 1970. \n",
    "\n",
    "With this in mind, what has made us choose this current project and build the network of songs of one of the most influential bands of all times is presented below. \n",
    "\n",
    "\n",
    "## <font color='darkgreen'> 1. Motivation   \n",
    "No band has influenced pop culture the way The Beatles have. They were integral to the development of 1960s counterculture and popular music's recognition as an art form.\n",
    "        \n",
    "Currenly, after 50 years of their retirement from the stage and with more than 200 songs on streaming servies, The Beatles' Songs are still present on a daily basis, still catchy and played all over the world, remaining forever and likley to be  passing down from generation to generation. But...\n",
    "    \n",
    "#### <center> What do the Lyrics of Beatles' Songs hide ? </center>\n",
    "    \n",
    "Thrilled with the idea, performing different analyses based on the lyrics of Beatles could reveal many interesting hidden facts over the career of the band. \n",
    "    \n",
    "    \n",
    "### <font color='green'> 1.1 _Our Dataset:_\n",
    "All the data used in this project were mainly extracted from three different Web sites. Next, they are presented together with the code used to build the dataset. \n",
    "    \n",
    "**1)** **[The Beatles official Page](https://www.thebeatles.com/)**. It has been of great relevance to be able to have a reliable list of all available Beatles Songs, representing the nodes of Beatles network, together with its corresponding lyrics. The lyrics were then used to define the edges of the network under a specific criteria. In that way, <u>Section 1.4.1</u>  shows the code to obtain the API of the Beatles song from their official page to extract the lyrics, consequently stored in a txt. file. \n",
    "    \n",
    "**2)** **[The Beatles Wiki](https://beatles.fandom.com/wiki/The_Beatles_Wiki)**. It was created in 2006, and edited some years after, currently containing a total amount of 775 pages. It has represented our main source to extract information from the available wiki-pages of Beatles songs, such as its songwriter, its corresponding album and its release date to be used as the node attributes of the network. <u>Section 1.4.2</u>  presents the code to obtain the API of the Wiki-Pages of each Beatles song, which were consequently stored in a txt. file to extract all the necessary information. Nevertheless, few Wiki-pages of each corresponding song had some features missing, reason why the other two Web sites were used to counter that point. \n",
    "\n",
    "**3)** **[The Beatles Spotify Page](https://open.spotify.com/artist/3WrFJ7ztbogyGnTHbHJFl2)**. Their spotify profile was thought that could be used to find other minor attributes of each song related to music characterstics, such as the danceability, the key or tempo of each track. In <u>Section 1.4.3</u>, we present how the API of the Beatles Spotify Page was obtained. \n",
    "\n",
    "After considerable data extraction from txt. file of the different sources, its consequent cleaning and preprocessing steps _(all detailed in Section 2)_, we ended up with a total number of **301 songs** to be able to build the most exiciting Beatles network of all times.     \n",
    "    \n",
    "### <font color='green'> 1.2 _Why?_\n",
    "After inspecting the literature, not many analysis have been focused on analyzing Beatles songs from a linguistic point of view, without revealing what makes their songs catchy and how they influenced each other. Fascinated by Beatles success and its music so many years after of their release, we were motivated enough to undertake the project of builing a network of Beatles songs and understand how could be related from a lyrics point of view. \n",
    "    \n",
    "#### <center> Could we get into Beatles'mind ? </center>\n",
    "    \n",
    "### <font color='green'> 1.3 _Our Goal:_\n",
    "The main aim of this project was to **build a Network of Beatles Songs** to be able to perform a deep analysis of the following points:\n",
    "    \n",
    "- Inspect how are Beatles songs related and inspired by each others, by also considering its songwritter. \n",
    "- Find communities of songs and their common particularities. \n",
    "- Perform a sentimental analysis to see which and how the most recurrent topics evoled over time. \n",
    "- Examine network growth to reveal Beatles' stages and their most inspirational years. \n",
    "    \n",
    "Overall, the current project could provide insights into songs and group dynamics of the Beatles and reveal many other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2e938",
   "metadata": {},
   "source": [
    "### <font color='green'> 1.4 _Code for Data Obtention:_\n",
    "\n",
    "#### <font color='green'> 1.4.1 _Data Extraction from The Beatles Official Page:_\n",
    "    \n",
    "When considering The Beatles Official Page, a function `extract_html(url)` has been created to extract the necessary information from the Home Beatles page. Consequenly, making use of the following regular expressions patterns: (1) `'hreflang=\"en\">(.*?)</a>'` and  (2) `'chronological-date\">(.*?)</td>'`, a list of the title of songs presented in the Official Beatles page was obtained and its release date, respectively. \n",
    "Each release date was then stored in terms of the year, month and specific day of release, to be consequenty sorted from the first released songs to the last ones and stored in a csv. file for later use. \n",
    "\n",
    "From that Official Page, as presented below, **301 Beatles Songs** were obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have create a function to obtain the HTLM of the Main Page of Beatles Official Web-page- \n",
    "def extract_html(url):\n",
    "    page = urlopen(url)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"utf-8\")\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We obtain all the list of songs and we stored it in the Variable songs_Titles\n",
    "pattern1= 'hreflang=\"en\">(.*?)</a>'\n",
    "pattern2= 'chronological-date\">(.*?)</td>'\n",
    "\n",
    "songs_titles=[]\n",
    "years=[]\n",
    "months=[]\n",
    "days=[]\n",
    "\n",
    "#Number of subpages of the Home Beatles page. \n",
    "for i in range(0,9): \n",
    "    url = 'https://www.thebeatles.com/songs?page='+str(i)\n",
    "    html=extract_html(url)\n",
    "    a=re.findall(pattern1,html)\n",
    "    b=re.findall(pattern2,html)\n",
    "    \n",
    "    for el in a:\n",
    "        el=el.replace('&#039;', '\\'')\n",
    "        songs_titles.append(el)\n",
    "    \n",
    "    for date in b:\n",
    "        date=date.split(' ')\n",
    "        years.append(date[3])\n",
    "        months.append(date[2][:-1])\n",
    "        days.append(date[1][:-2])\n",
    "\n",
    "data={'Song':songs_titles,'Year':years,'Month':months,'Day':days}\n",
    "data_df=pd.DataFrame(data)\n",
    " \n",
    "\n",
    "#The name of the month was replaced by its corresponding number for ease of order songs by its release date. \n",
    "months_value={'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,\n",
    "             'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
    "\n",
    "months_sorted=data_df.replace({'Month': months_value}).sort_values(by=['Year','Month','Day']).index\n",
    "data_df=data_df.reindex(months_sorted)\n",
    "data_df=data_df.reset_index(drop=True)\n",
    "\n",
    "print('The official Beatles page contains information of a number of' , len(songs_titles), 'songs.')\n",
    "data_df.to_csv(\"Songs_Sorted_Years.csv\",index=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a148080",
   "metadata": {},
   "source": [
    "Secondly, the Beatles Official Page was also used to **extract the lyrics of the songs**, properly saved in a txt. file for further cleaning and pre-processing (_detailed in <u>Section 2.1</u>)_. In that way the following approach has been adopted:\n",
    "\n",
    "1. Going through the title of all songs, the API from each web-page was again obtained to save all raw data in a txt. file, one for each song. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a65d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[' a ',' in ',' the ',' of ',' to ',' is ',' at ',' for ',' that ',' by ',' as ',' from ',' into ',\n",
    "             ' on ',' with ',' off ',' this ',' up ',' like ']\n",
    "remove_list_start=['a-','in-','the-','of-','to-','is-','at-','for-','that-','by-','as-','from-','like-','this-',\n",
    "                  'with-']\n",
    "remove_list_end=['-by','-to','-on','-is','-that','-before']\n",
    "remove_list2=['\\'','.','!','(',')',',','/',':']\n",
    "\n",
    "songs_titles2=[]\n",
    "for title in songs_titles:\n",
    "    try:\n",
    "        title=title.lower()\n",
    "        for el in remove_list2: # delete special characters\n",
    "            title=title.replace(el,'')\n",
    "        for el in remove_list: # replace single words with a space\n",
    "            title=title.replace(el,' ')\n",
    "        title=title.replace(' ','-')\n",
    "        title=title.replace('--','-')\n",
    "        # delete words from the start\n",
    "        for i in range(0,6):\n",
    "            if title[:i] in remove_list_start:\n",
    "                title=title[i:]\n",
    "        # delete words from the end\n",
    "        for i in range(0,8):\n",
    "            if title[-i:] in remove_list_end:\n",
    "                title=title[:-i]\n",
    "        if title[0]=='-':\n",
    "            title=title[1:]\n",
    "        url='https://www.thebeatles.com/'\n",
    "        query=extract_html(url+title)\n",
    "        # write the extracted text in a .txt file\n",
    "        file=open('raw_songs/'+title+'.txt','w+')\n",
    "        file.write(query)\n",
    "        file.close()\n",
    "        songs_titles2.append(title)\n",
    "        \n",
    "    #To know the songs where the url could not be obtained. \n",
    "    except:\n",
    "        print(\"The URL of\", title, \"song could not be obtained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c8b41",
   "metadata": {},
   "source": [
    "As the \"words-love\" song could not be obtained due to the different format of its query, we manually stored its content. \n",
    "\n",
    "2. With all the raw text from each page of Beatles Song, the so-called function `extract_lyrics(song_path)` was created with the aim to just extracted the text with the lyrics part. Using the regular expressions detailed in `pattern3` and `pattern4`, we extracted the text between them and stored it in another txt. file.  Provided that the song has no lyrics, we classified it as an instrumental song and was not considered as part of our lyrics network.\n",
    "\n",
    "As presented below, after lyrics extraction, we ended up with **197 songs with lyrics** and **103 instrumental songs**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef06a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3='<div class=\"col-md-6 middle-content border-left border-right\"><p>'\n",
    "pattern4='<figure class=\"wp-block-table table-expander table table-imported\">'\n",
    "remove_list=['<br />','\\n','</p>','<p>']\n",
    "\n",
    "def extract_lyrics(song_path):\n",
    "    song=open(song_path).read()\n",
    "    idx_init = re.search(pattern3,song).end()\n",
    "    idx_final = re.search(pattern4,song).start()\n",
    "    lyrics=song[idx_init:idx_final]\n",
    "    for el in remove_list: # delete special characters\n",
    "        lyrics=lyrics.replace(el,' ')\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = os.listdir('raw_songs')\n",
    "songs_no_lyrics=[]\n",
    "for el in txt_files:\n",
    "    el=el[:-4]\n",
    "    song_path='raw_songs/'+el+'.txt'\n",
    "    try:\n",
    "        lyrics=extract_lyrics(song_path)\n",
    "        file=open('lyrics_songs/'+el+'.txt','w+')\n",
    "        file.write(lyrics)\n",
    "        file.close()\n",
    "    except:\n",
    "        songs_no_lyrics.append(el)\n",
    "\n",
    "        \n",
    "lyrics_files = os.listdir('lyrics_songs')\n",
    "for el in songs_no_lyrics:\n",
    "    el=el[:-4]\n",
    "    url='https://www.thebeatles.com/'\n",
    "    # check if the song has lyrics or not\n",
    "    for i in range(0,4):\n",
    "        try:\n",
    "            query=extract_html(url+el+'-'+str(i))\n",
    "            idx_init = re.search(pattern6,query).end()\n",
    "            idx_final = re.search(pattern7,query).start()\n",
    "            # rewrite the extracted text in a .txt file\n",
    "            file=open('raw_songs/'+el+'.txt','w+')\n",
    "            file.write(query)\n",
    "            file.close()\n",
    "            print(el, i)\n",
    "            break\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10dfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add as an attribute of the songs whether it is an instrumental song or if it has lyrics. \n",
    "songs = pd.read_csv(\"Songs_Sorted_Years.csv\")  \n",
    "DataSongs=pd.DataFrame(songs)\n",
    "\n",
    "lyrics_bool=[]\n",
    "for song in list(DataSongs['Song']):\n",
    "    if song in songs_no_lyrics:\n",
    "        lyrics_bool.append('No')\n",
    "    else:\n",
    "        lyrics_bool.append('Yes')\n",
    "        \n",
    "DataSongs['Lyrics']=lyrics_bool\n",
    "DataSongs.to_csv('Songs_Sorted_Years.csv',index=False)\n",
    "\n",
    "print('The official Beatles page contains' , len(lyrics_files)+len(songs_no_lyrics), 'Songs, with',len(lyrics_files),\n",
    "     'Lyrics Songs and',len(songs_no_lyrics),'Instrumental ones. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9f7f4",
   "metadata": {},
   "source": [
    "#### <font color='green'> 1.4.2 _Data Extraction from The Beatles Wiki:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b7830",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2721930",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"Songs_Sorted_Years.csv\")  \n",
    "DataSongs=pd.DataFrame(songs)\n",
    "\n",
    "\n",
    "songs_list=list(DataSongs.iloc[:,0])\n",
    "songs_list_clean=[]\n",
    "for el in songs_list: \n",
    "    el=el.replace('-','_')\n",
    "    el_song=list(el)\n",
    "    indexes=[]\n",
    "    \n",
    "    for ind, a in enumerate(el_song):\n",
    "        if ind==0:\n",
    "            indexes.append(ind) \n",
    "        if a=='_':\n",
    "            indexes.append(ind+1)\n",
    "            \n",
    "    indexes_set=set(indexes)\n",
    "    \n",
    "    for i in indexes_set:\n",
    "        el_song[i] = el_song[i].upper()\n",
    "\n",
    "    string = ''.join(el_song)\n",
    "    songs_list_clean.append(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITER: Lennon&McCartney, McCartney, Lennon, Harrison, Ringo\n",
    "\n",
    "writer=np.zeros(len(DataSongs))\n",
    "album=np.zeros(len(DataSongs))\n",
    "genre=np.zeros(len(DataSongs))\n",
    "\n",
    "DataSongs['Writer']=writer\n",
    "DataSongs['Album']=album\n",
    "DataSongs['Song']=songs_list_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://beatles.fandom.com/api.php?\"\n",
    "action = \"action=query\"  \n",
    "content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "dataformat =\"format=json\"\n",
    "\n",
    "for a in songs_list_clean:\n",
    "    try:\n",
    "        title = \"titles=\"+str(a)\n",
    "        Beatles = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title,dataformat)\n",
    "        urllib2.urlretrieve(Beatles , 'wiki_songs/'+str(a)+'.txt')\n",
    "    except:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path=\"./wiki_songs/\"\n",
    "\n",
    "for a in songs_list_clean:\n",
    "    with open(pages_path+a+\".txt\",\"r\") as f:\n",
    "        wiki_song= f.read()\n",
    "        writer= re.findall(r'\\[\\[Category:Songs written by (.*?)\\]\\]', wiki_song)\n",
    "        genre= re.findall(r'genre=(.*?)\\\\n',wiki_song)\n",
    "        \n",
    "        if genre: \n",
    "            DataSongs['Genre'][index]= genre[0]\n",
    "\n",
    "        if writer:\n",
    "            if writer[0]==\"Lennon\\\\u2013McCartney\":\n",
    "                index=DataSongs.index[DataSongs['Song']==a].tolist()       \n",
    "                DataSongs['Writer'][index]= 'Lennon/McCartney'\n",
    "                \n",
    "            else:\n",
    "                index=DataSongs.index[DataSongs['Song']==a].tolist() \n",
    "                DataSongs['Writer'][index]= writer[0]\n",
    "                \n",
    "\n",
    "        else:\n",
    "            index=DataSongs.index[DataSongs['Song']==a].tolist()       \n",
    "            DataSongs['Writer'][index]= 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "refill_songs=[]\n",
    "for a in songs_list_clean:\n",
    "    with open(pages_path+a+\".txt\",\"r\") as f:\n",
    "        wiki_song= f.read()\n",
    "        writer= re.findall(r'\\[\\[Category:Songs written by (.*?)\\]\\]', wiki_song)\n",
    "        \n",
    "        if writer: #82 songs with writer\n",
    "            i=i+1\n",
    "            if writer[0]==\"Lennon\\\\u2013McCartney\":\n",
    "                index=DataSongs.index[DataSongs['Song']==a].tolist()       \n",
    "                DataSongs['Writer'][index]= 'Lennon/McCartney'\n",
    "                \n",
    "            else:\n",
    "                index=DataSongs.index[DataSongs['Song']==a].tolist() \n",
    "                DataSongs['Writer'][index]= writer[0]\n",
    "                \n",
    "        else: #219 missing\n",
    "            refill_songs.append(a)\n",
    "            index=DataSongs.index[DataSongs['Song']==a].tolist()       \n",
    "            DataSongs['Writer'][index]= 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to save all the Wiki Pages related to albums. \n",
    "Albums_pages= ['Sgt._Pepper%27s_Lonely_Hearts_Club_Band_(album)','Let_It_Be_(album)','Hey_Jude_(album)',\n",
    "               'Help!_(album)','A_Hard_Day%27s_Night_(album)','With_the_Beatles','Please_Please_Me_(album)',\n",
    "              'Beatles_for_Sale','Yellow_Submarine_Songtrack','Introducing...The_Beatles','Yesterday_and_Today']\n",
    "Albums_name=['Sgt_Peppers_Lonely_Hearts_Club_Band','Let_It_Be','Hey_Jude','Help!','A_Hard_Days_Night',\n",
    "             'With_the_Beatles','Please_Please_Me','Beatles_for_Sale','Yellow_Submarine_Songtrack',\n",
    "            'Introducing...The_Beatles','Yesterday_and_Today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Album Wiki-Pages\n",
    "for page in Albums_pages:\n",
    "    title = \"titles=\"+str(page)\n",
    "    Album_Page = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title,dataformat)\n",
    "    urllib2.urlretrieve(Album_Page  , 'wiki_albums/'+str(page)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22dbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path=\"./wiki_albums/\"\n",
    "artists=['Lennon/McCartney','Paul McCartney|McCartney','Lennon/McCartney|Lennon with McCartney','George Harrison',\n",
    "         'George Harrison|Harrison','John Lennon','John Lennon|Lennon','Ringo Starr|Starkey','Ringo Starr|Starr',\n",
    "        'Chuck Berry','Lennon/McCartney|McCartney with Lennon','Lennon/McCartney|McCartney/Lennon',\n",
    "        'Little Richard|Richard Penniman','George Martin','Vee Jay Records|Vee Jay','Paul McCartney',\n",
    "        'Ringo Starr|Richard Starkey']\n",
    "\n",
    "#General Regular Expression for Wiki-Pages of all albums\n",
    "Tracks='(?:==Tracks==|==Track listing==)'\n",
    "TotalLength='Total Length'\n",
    "Credits='(?:Credits|==References==|==American release==|==Personnel==|==External links==)'\n",
    "pattern_songs_albums=\"\\[\\[(.*?)\\]\\]\"\n",
    "\n",
    "Songs_Albums=[] #Array to store all the Songs from an album\n",
    "All_Years=[]\n",
    "All_Albums=[]\n",
    "\n",
    "for j,page in enumerate(Albums_pages):\n",
    "    print(page)\n",
    "    with open(pages_path+page+\".txt\",\"r\") as f:\n",
    "        wiki_album= f.read()\n",
    "        \n",
    "        #Release Date of the Album\n",
    "        album_drelease= re.findall(r'release date(.*?)recorded',wiki_album)\n",
    "        if album_drelease:\n",
    "            album_yrelease= re.findall(r'\\d\\d\\d\\d',album_drelease[0])\n",
    "        else:\n",
    "            album_yrelease= re.findall(r'19\\d\\d',wiki_album)\n",
    "        \n",
    "            \n",
    "        songs=re.findall(r\"\\[\\[(.*?)\\]\\]\",wiki_album)\n",
    " \n",
    "\n",
    "        a=[(m.start(0), m.end(0)) for m in re.finditer(Tracks, wiki_album)]\n",
    "        if len(a)!=1:\n",
    "            a=a[1]\n",
    "        else:\n",
    "            a=a[0]\n",
    "        b=[(m.start(0), m.end(0)) for m in re.finditer(TotalLength, wiki_album)]\n",
    "        if len(b)>0:\n",
    "            b=b[1]\n",
    "        else:\n",
    "            c=[(m.start(0), m.end(0)) for m in re.finditer(Credits, wiki_album)]\n",
    "            if len(c)>0:\n",
    "                b=c[0]\n",
    "            else:\n",
    "                b=(a[0]+600,a[1]+600)\n",
    "\n",
    "\n",
    "        ind_songs=[(m.start(0), m.end(0)) for m in re.finditer(pattern_songs_albums, wiki_album)]\n",
    "        songs=re.findall(r\"\\[\\[(.*?)\\]\\]\",wiki_album)\n",
    "        \n",
    "        Songs=[]\n",
    "        for i,count in enumerate(ind_songs):\n",
    "            if count[0] > a[1] and count[0] < b[0]:\n",
    "                if songs[i] not in artists:\n",
    "                    if re.findall(r\"song\",songs[i]):\n",
    "                        r=[(m.start(0), m.end(0)) for m in re.finditer(\"song\", songs[i])]\n",
    "                        Songs.append(songs[i][0:r[0][0]-1])\n",
    "                        \n",
    "                    else:\n",
    "                        if page=='Introducing...The_Beatles':\n",
    "                            no_include=['Ask Me Why',\"Please Please Me\"]\n",
    "                            \n",
    "                            if songs[i] not in no_include:\n",
    "                                Songs.append(songs[i])\n",
    "                                \n",
    "                        else:\n",
    "                            Songs.append(songs[i])\n",
    "                  \n",
    "        \n",
    "        year=np.full(len(Songs), album_yrelease[0])\n",
    "        All_Years.extend(year)\n",
    "        album=np.full(len(Songs), Albums_name[j])\n",
    "        All_Albums.extend(album)\n",
    "        Songs_Albums.extend(Songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAlbums= pd.DataFrame(Songs_Albums,columns=['Songs']) \n",
    "DataAlbums['Album']=All_Albums\n",
    "DataAlbums['Release Year']=All_Years\n",
    "\n",
    "DataAlbums.to_csv('AllAlbums.txt',index=False)\n",
    "DataSongs.to_csv('AllAtributes.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf5bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46028df",
   "metadata": {},
   "source": [
    "#### <font color='green'> 1.4.3 _Data Extraction from The Beatles Spotify Page:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f956e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTIPY_CLIENT_ID = \"fd50ff911d1d46079f0df6b9e7c71572\"\n",
    "SPOTIPY_CLIENT_SECRET = \"2ecb8564def04ead945c43c9d8343b3d\"\n",
    "BEATLES_ID='3WrFJ7ztbogyGnTHbHJFl2'\n",
    "BEATLES_PLAYLIST_ID='37i9dQZF1DXdLtD0qszB1w'\n",
    "SpotifyClientCredentials(SPOTIPY_CLIENT_ID,SPOTIPY_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e747e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "birdy_uri = 'spotify:artist:3WrFJ7ztbogyGnTHbHJFl2'\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(SPOTIPY_CLIENT_ID,SPOTIPY_CLIENT_SECRET))\n",
    "\n",
    "results = spotify.artist_albums(birdy_uri, album_type='album')\n",
    "albums = results['items']\n",
    "while results['next']:\n",
    "    results = spotify.next(results)\n",
    "    albums.extend(results['items'])\n",
    "\n",
    "for album in albums:\n",
    "    print(album['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc433d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "489cd657",
   "metadata": {},
   "source": [
    "#### <font color='green'> 1.4.4 _To an unique Dataset:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Songs = pd.read_csv(\"AllAtributes.txt\") \n",
    "DataSongs=pd.DataFrame(Songs)\n",
    "\n",
    "Albums = pd.read_csv(\"AllAlbums.txt\")\n",
    "DataAlbums=pd.DataFrame(Albums)\n",
    "\n",
    "AllSongs = pd.read_csv(\"Beatles_Dataset.csv\")\n",
    "DataAllSongs=pd.DataFrame(AllSongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aaf694",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=[]\n",
    "\n",
    "for indS,a in enumerate(DataSongs['Song']):\n",
    "    \n",
    "    a=a.replace('_',' ')\n",
    "        \n",
    "    #Complete Artist\n",
    "    if DataSongs['Writer'][indS]=='Not found':\n",
    "        if a in list(DataAllSongs['Title']): \n",
    "            indW=DataAllSongs.index[DataAllSongs['Title']==a].tolist()[0]\n",
    "            DataSongs['Writer'][indS]=DataAllSongs['Songwriter'][indW]\n",
    "        else:\n",
    "            actual.append(a)\n",
    "\n",
    "    #Complete Albums\n",
    "    if a in list(DataAlbums['Songs']):\n",
    "        indA=DataAlbums.index[DataAlbums['Songs']==a].tolist()[0]\n",
    "        DataSongs['Album'][indS]=DataAlbums['Album'][indA]\n",
    "    else:\n",
    "        if a in list(DataAllSongs['Title']):\n",
    "            indAA=DataAllSongs.index[DataAllSongs['Title']==a].tolist()[0]\n",
    "            DataSongs['Album'][indS]=DataAllSongs['Album.debut'][indAA]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc535389",
   "metadata": {},
   "source": [
    "## <font color='darkgreen'> 2. Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b66428",
   "metadata": {},
   "source": [
    "### <font color='green'> 2.1 _Data Cleaning and Preprocessing:_\n",
    "\n",
    "        \n",
    "Once the lyrics had been extracted, it was needed to clean it as much as possible. With cleaning, it is meant deleting all those words and characters that do not add any special information to the text. It would not make sense to connect two songs just because they share a preposition or a conjunction, which are mainly the most common words in any language.\n",
    "\n",
    "Thus, in order to achive so, the _clean_lyrics( )_ function shown below has been created.\n",
    "\n",
    "    \n",
    "What this function basically does is first clean and then tokenize the lyrics. To accomplish the cleaning part, first it removes any punctuation sign from the lyrics. Then it removes anys special characters that had been left when extracting it form the webpages api, such as _\\n_, =_ or empty spaces. Finally, it sets every word to lower case, and deletes all that ones that are not contained in the _nltk.corpus_ stopswords library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ce16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words = [el.replace('\\'','') for el in stop_words]\n",
    "stop_words.append('im')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0710cd",
   "metadata": {},
   "source": [
    "<font color='darkblue'> A function to **clean the lyrics** is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(file_path):\n",
    "    data=open(file_path).read()\n",
    "    # import WordPunctTokenizer() method from nltk\n",
    "    # Create a reference variable for Class WordPunctTokenizer\n",
    "    tk = WordPunctTokenizer()\n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in data:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    # Remove all the special characters as \\n and single = left\n",
    "    char=['\\n','=']\n",
    "    for el in char:\n",
    "        raw=re.sub(el,'',no_punct)\n",
    "    # remove stop words\n",
    "    token_txt = tk.tokenize(raw.lower()) # set to lower case\n",
    "    token_txt = tk.tokenize(raw.lower()) # set to lower case\n",
    "    token_final = [x for x in token_txt if x not in stop_words and len(x)>2]\n",
    "    return token_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49499fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    remove_list=[' a ',' in ',' the ',' of ',' to ',' is ',' at ',' for ',' that ',' by ',' as ',' from ',' into ',\n",
    "                 ' on ',' with ',' off ',' this ',' up ',' like ']\n",
    "    remove_list_start=['a-','in-','the-','of-','to-','is-','at-','for-','that-','by-','as-','from-','like-','this-',\n",
    "                      'with-']\n",
    "    remove_list_end=['-by','-to','-on','-is','-that','-before']\n",
    "    remove_list2=['\\'','.','!','(',')',',','/',':','\\‚Äô','?']\n",
    "\n",
    "    pattern6='<div class=\"col-md-6 middle-content border-left border-right\"><p>'\n",
    "    pattern7='<figure class=\"wp-block-table table-expander table table-imported\">'\n",
    "    try:\n",
    "        title=title.lower()\n",
    "        for el in remove_list2: # delete special characters\n",
    "            title=title.replace(el,'')\n",
    "        for el in remove_list: # replace single words with a space\n",
    "            title=title.replace(el,' ')\n",
    "        title=title.replace(' ','-')\n",
    "        title=title.replace('--','-')\n",
    "        # delete words from the start\n",
    "        for i in range(0,6):\n",
    "            if title[:i] in remove_list_start:\n",
    "                title=title[i:]\n",
    "        # delete words end\n",
    "        for i in range(0,8):\n",
    "            if title[-i:] in remove_list_end:\n",
    "                title=title[:-i]\n",
    "        if title[0]=='-':\n",
    "            title=title[1:]\n",
    "        return title\n",
    "    except:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_files=[el[:-4] for el in os.listdir('lyrics_songs')]\n",
    "lyrics_files.remove('.DS_S') # remove this element that is introduced when using os.listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f23b5f",
   "metadata": {},
   "source": [
    "## <font color='darkgreen'> 3. Tools, theory and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d76ada",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.1 _Builing the Network:_\n",
    "    \n",
    "In order to accomplish one of the main goals of this project, that was connecting songs (nodes) between them depending on if they shared one at least of their common words according to tdf-if index, special emphasis has been put on text processing. Here we must distinguish between data extraction and preprocessing on one side, and sentimental analysis on the other. Let's focus now on the first part.\n",
    "    \n",
    "    \n",
    "To connect the links we decided to go a step further and establish the linking criteria not on which were the most common words from each song, but on which were the five words in each song with higher tc-idf score. To do so, basically tc and idf has been computed, to finally compute tc-idf for each word and each song while storing the values in a dictionary. Then the dicctionary was sorted according to its values, and thus the five words with higher tc-idf score values were picked for each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tc(corpus):\n",
    "    \"\"\" Calculates the term count for each word \"\"\"\n",
    "    tc_dict = {}\n",
    "    for song in corpus.fileids():\n",
    "        text = nltk.Text(corpus.words(song))\n",
    "        tc_dict[song[:-4]] = dict(FreqDist(text))\n",
    "    return tc_dict\n",
    "\n",
    "def idf(corpus):\n",
    "    \"\"\" Calculates the IDF for each word in the corpus\"\"\"\n",
    "    text = TextCollection(corpus)\n",
    "    idf_dict = {}\n",
    "    for word in set(corpus.words()):\n",
    "        idf_dict[word] = text.idf(word)\n",
    "    return idf_dict\n",
    "\n",
    "def tc_idf(corpus):\n",
    "    \"\"\" Calculates the TC-IDF for each word in the corpus, and returns it as a dictionary \"\"\"\n",
    "    tc_dict = tc(corpus)\n",
    "    idf_dict = idf(corpus)\n",
    "    tc_idf_dict = {}\n",
    "    songs = list(tc_dict.keys())\n",
    "    for song in songs:\n",
    "        tc_idf_dict[song] = {}\n",
    "        for key, value in tc_dict[song].items():\n",
    "            tc_idf_dict[song][key] = value * idf_dict[key]\n",
    "    return tc_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0956b",
   "metadata": {},
   "source": [
    "We save the tc-idf values of the corpus as a dict, and we order each song from the lower to the higher tc-idf value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcde750",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_idf_dict=tc_idf(corpus)\n",
    "for el in tc_idf_dict:\n",
    "    tc_idf_dict[el]=sorted(tc_idf_dict[el].items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29147903",
   "metadata": {},
   "source": [
    "We save the top 5 words for each song according to the tc-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_idf_top5={}\n",
    "for el in tc_idf_dict:\n",
    "    tc_idf_top5[el]=[word[0] for word in tc_idf_dict[el][-2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a16637",
   "metadata": {},
   "source": [
    "Now let's create a dictionary to store the links.\n",
    "To create the hyperlinks a double loop has been created, iterating over all the songs. So, when the external loop picks a song, the inner loop iterates all over the songs, comparing each inner song with the external one. Thus, if between each pair of songs a word is shared, it is stored as they have equal word/s in common. Finally, to decide how the links are directioned, we pick the _Year_ attribute from both songs. Between each pair of songs, the song that was realeased earlier will point towards the other one.\n",
    "\n",
    "Once all this has been done, the network is created by using the _nx.DiGraph()_ command. Each song name is added as a node, an each link stored before is added as mentioned above. Finally, to perform a better analysis of how the songs are related with, the GCC component is extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyperlinks(lyrics_files,tc_idf_top5):\n",
    "    hyperlinks={} #dictionary to store links \n",
    "\n",
    "    for file1 in lyrics_files:\n",
    "        eq_files=[]\n",
    "        for file2 in lyrics_files:\n",
    "            if file1!=file2:\n",
    "                a=tc_idf_top5[file1]\n",
    "                b=tc_idf_top5[file2]\n",
    "                # compute if there are equal words in both lists\n",
    "                eq=0\n",
    "\n",
    "                for el in a:\n",
    "                    if el in b:\n",
    "                        eq+=1\n",
    "                if eq!=0:\n",
    "                    eq_files.append(file2)\n",
    "\n",
    "        idx1=Song_data[Song_data['Song']==file1].index[0]\n",
    "\n",
    "        eq_files_later=[]\n",
    "\n",
    "        for eq in eq_files:\n",
    "            idx2=Song_data[Song_data['Song']==eq].index[0]\n",
    "            if idx1<idx2: # means that file1 song was released earlier\n",
    "                eq_files_later.append(eq)\n",
    "\n",
    "        hyperlinks[file1]=eq_files_later\n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48d8ce",
   "metadata": {},
   "source": [
    "<font color='darkblue'> With the links stored, we are able to build the directed Graph based on the date release of the corresponding songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d31f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(lyrics_files,hyperlinks):\n",
    "    #We build the Directed Network\n",
    "    Beatles_nw= nx.DiGraph()\n",
    "    for file in lyrics_files:\n",
    "        #We add the nodes and attributes to the the network\n",
    "        file=file.replace('-',' ')\n",
    "        Beatles_nw.add_node(file)\n",
    "\n",
    "    #We add the hyperlinks to the the network\n",
    "    for el in lyrics_files:\n",
    "        links= hyperlinks[el]\n",
    "        for a in links:\n",
    "            u= el\n",
    "            v= a\n",
    "            u=u.replace('-',' ')\n",
    "            v=v.replace('-',' ')\n",
    "            Beatles_nw.add_edge(u,v)\n",
    "    return Beatles_nw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a8041",
   "metadata": {},
   "source": [
    "<font color='darkblue'> Next, we extract the **GCC** subgraph of the Beatles network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_GCC(Beatles_nw):\n",
    "    Gs = list(Beatles_nw.subgraph(c).copy() for c in nx.weakly_connected_components(Beatles_nw))\n",
    "    biggest = 0\n",
    "    GCC_index = 0\n",
    "    for index,graph in enumerate(Gs):\n",
    "        if len(graph.nodes) > biggest:\n",
    "            biggest = len(graph.nodes)\n",
    "            GCC_index = index\n",
    "    GCC = Gs[GCC_index]\n",
    "    print('- Considering the GCC of the Beatles Network, it has',len(GCC.nodes()),'number of nodes and',\n",
    "      len(GCC.edges.data()),'number of links.')\n",
    "    return GCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874f9b7",
   "metadata": {},
   "source": [
    "To determine the criteria of how many words should be chosen according to the tc-idf score when establishing the links between nodes, we will prioratize the threshold that gives us higher modularity when finding communities in our songs\n",
    "\n",
    "\n",
    "    - Determining a criteria about which should be the accurate number of words chosen (no crec q calgui posar totes les imatges i si si, ho faria amb un subplot maybe)\n",
    "    \n",
    "To determine the criteria of how many words should be chosen according to the tc-idf score when establishing the links between nodes, an optimal point had to be found between prioratizing the threshold that gave us higher modularity when aiming to find communities in our songs, and obtaining a sufficient of nodes and links that makes the analysis of the network significant.\n",
    "\n",
    "To do so, a loop has been created to see how the network size and shape evolves as the number of words per song to establish the comparison increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "modularities=[]\n",
    "for i in range(2,10):\n",
    "    print('Threshold: ',i)\n",
    "    tc_idf_top={}\n",
    "    for el in tc_idf_dict:\n",
    "        tc_idf_top[el]=[word[0] for word in tc_idf_dict[el][-i:]]\n",
    "    hyperlinks=create_hyperlinks(lyrics_files,tc_idf_top)\n",
    "    nw=build_network(lyrics_files,hyperlinks)\n",
    "    GCC=extract_GCC(nw)\n",
    "    # Compute the best partition. We need to take into account the undirected graph. \n",
    "    un_Beatles_nw= GCC.to_undirected()\n",
    "    partition = community.community_louvain.best_partition(un_Beatles_nw)\n",
    "    print('Number of communities found: ', len(np.unique(list(partition.values()))))\n",
    "    mod = community.community_louvain.modularity(partition, un_Beatles_nw)\n",
    "    print(\"The modularity value is:\",\"{:.3f}\".format(mod))\n",
    "    modularities.append(mod)\n",
    "    node_community = [node[1] for node in partition.items()]\n",
    "    pCol = {i: list(np.random.random(size=3)) for i in set(node_community)}\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    forceatlas2 = ForceAtlas2(\n",
    "                            # Behavior alternatives\n",
    "                            outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                            linLogMode=False,  # NOT IMPLEMENTED\n",
    "                            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                            edgeWeightInfluence=2.0,\n",
    "\n",
    "                            # Performance\n",
    "                            jitterTolerance=1.0,  # Tolerance\n",
    "                            barnesHutOptimize=True,\n",
    "                            barnesHutTheta=1.2,\n",
    "                            multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                            # Tuning\n",
    "                            scalingRatio=0.1,\n",
    "                            strongGravityMode=False,\n",
    "                            gravity=10.0,\n",
    "\n",
    "                            # Log\n",
    "                            verbose=False)\n",
    "\n",
    "    positions = forceatlas2.forceatlas2_networkx_layout(un_Beatles_nw, pos=None, iterations=2000)\n",
    "    f = dict(nx.degree(un_Beatles_nw))\n",
    "    node_sizes = [v*100 for v in f.values()]\n",
    "    print(\"The distribution of the community sizes\")\n",
    "    cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "    fig = plt.gcf()\n",
    "    nx.draw(un_Beatles_nw, positions, node_size=node_sizes, with_labels=False, width = 0.35, node_color=list(partition.values()), alpha=0.9)\n",
    "    fig.set_size_inches(20, 15)\n",
    "    # plt.savefig('fignet'+str(i))\n",
    "    plt.show()\n",
    "        \n",
    "plt.plot(list(range(2,10)),modularities)\n",
    "plt.xlabel('Number of words threshold')\n",
    "plt.ylabel('Modularity')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec6891",
   "metadata": {},
   "source": [
    "Let's consider we put the threshold at 5.\n",
    "\n",
    "**From the graphs and data plotted above, it has been decided to put the threshold at 5 words. Thus, achieving both a high level of modularity and a significant number of links and nodes when considering the GCC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_idf_top={}\n",
    "for el in tc_idf_dict:\n",
    "    tc_idf_top[el]=[word[0] for word in tc_idf_dict[el][-5:]]\n",
    "hyperlinks=create_hyperlinks(lyrics_files,tc_idf_top)\n",
    "nw=build_network(lyrics_files,hyperlinks)\n",
    "GCC=extract_GCC(nw)\n",
    "# Compute the best partition. We need to take into account the undirected graph. \n",
    "un_Beatles_nw= GCC.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b1011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6657bcab",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.2 _Visualize the Network:_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad6cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7be5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54643319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing node size depending on the degree of each node. \n",
    "degree=[el *15 for el in list(dict(GCC.out_degree()).values())]\n",
    "\n",
    "\n",
    "#Specifying Labels\n",
    "highest_degree = dict(sorted(GCC.out_degree, key=lambda x: x[1], reverse=True)[0:20])\n",
    "labels_draw = list(highest_degree.keys())\n",
    "labels = {}    \n",
    "for node in GCC.nodes():\n",
    "    if node in labels_draw:\n",
    "        labels[node] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba39455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get node positions based on the Force Atlas 2 algorithm \n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.5,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=0.01,#1 before - 0.01 after\n",
    "                        strongGravityMode=False,#False  before - true after\n",
    "                        gravity=10, #1.0 before -1.5 after\n",
    "\n",
    "                        # Log\n",
    "                        verbose=False) #True before - false after\n",
    "\n",
    "#getting node positions\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(GCC, pos=None, iterations=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caeede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION\n",
    "options = {\n",
    "    'node_size': degree,\n",
    "    'width': 0.2,\n",
    "    #'edge_color':edge_colors,\n",
    "    'node_color': 'orange',\n",
    "    #'labels':labels,\n",
    "    'alpha':0.9\n",
    "}\n",
    "\n",
    "fig = plt.gcf()\n",
    "nx.draw(GCC, positions,**options) \n",
    "fig.set_size_inches(20, 15)\n",
    "plt.savefig(\"Final_Graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a9714",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.3 _Statistical Analysis of the Network:_\n",
    "    \n",
    "- **Degree Distributions:**\n",
    "First, some basic stats of the network have been computed. Thus, total degree, in-degree and out-degree distributions are plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = sorted(GCC.degree(GCC.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "degrees_dist = [tuples[1] for tuples in degrees]\n",
    "v1 = np.arange(int(min(degrees_dist )),int(max(degrees_dist))+4,1)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "count1,bins1=np.histogram(degrees_dist ,bins=v1)\n",
    "plt.bar(bins1[:-1], count1,color='darkorange')\n",
    "plt.title('Distribution of degrees of the Beatles Song Network',fontsize=10)\n",
    "plt.xlabel('Degree',fontsize=12,fontweight='bold')\n",
    "plt.ylabel('Count',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb50cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot of the in-degree distribution\n",
    "in_degrees = sorted(GCC.in_degree(GCC.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "in_degrees_dist = [tuples[1] for tuples in in_degrees]\n",
    "v2 = np.arange(int(min(in_degrees_dist )),int(max(in_degrees_dist)),2)\n",
    "\n",
    "fig2 = plt.gcf()\n",
    "count2,bins2=np.histogram(in_degrees_dist ,bins=v2)\n",
    "plt.bar(bins2[:-1], count2,color='darkorange')\n",
    "plt.title('Distribution of in-degrees of the Beatles Song Network',fontsize=10)\n",
    "plt.xlabel('In-Degree',fontsize=12,fontweight='bold')\n",
    "plt.ylabel('Count',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b08950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot of the out-degree distribution\n",
    "out_degrees = sorted(GCC.out_degree(GCC.nodes, weight=None), key=lambda x: x[1], reverse = True)\n",
    "out_degrees_dist = [tuples[1] for tuples in out_degrees]\n",
    "\n",
    "fig3 = plt.gcf()\n",
    "v3 = np.arange(int(min(out_degrees_dist)),int(max(out_degrees_dist)),2)\n",
    "count3,bins3=np.histogram(out_degrees_dist,bins=v3)\n",
    "plt.bar(bins3[:-1], count3,color='orange')\n",
    "plt.title('Distribution of out-degrees of the Beatles Song Network',fontsize=10)\n",
    "plt.xlabel('Out-Degree',fontsize=12,fontweight='bold')\n",
    "plt.ylabel('Counts',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e83bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a39b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fde500b",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.4 _Finding Communities:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976879b",
   "metadata": {},
   "source": [
    "In order to find communities inside the created network _community.community_louvain.best_partition()_ algorithm has been used. Thus the GCC of the beatles networks has been converted to undirected and inserted as an input to this algorithm. The results found show that 9 communities have been found, with a modularity value of 0.595.\n",
    "\n",
    "\n",
    "Separation between communities might be useful to analyze how songs are related between each other when establishing this linking method, and thus analyze which are the common characteristics and peculiarities of each community. First we started computing which were the most common words used in each community considering both the total number of words and the unique ones. Results are shown in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the best partition. We need to take into account the undirected graph. \n",
    "un_Beatles_nw= GCC.to_undirected()\n",
    "while len(np.unique(list(partition.values())))!=9:\n",
    "    partition = community.community_louvain.best_partition(un_Beatles_nw)\n",
    "\n",
    "print('Number of communities found: ', len(np.unique(list(partition.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = community.community_louvain.modularity(partition, un_Beatles_nw)\n",
    "print(\"The modularity value is:\",\"{:.3f}\".format(mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6a93d",
   "metadata": {},
   "source": [
    "Compute the number of songs in every community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fff959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the number of songs in every community\n",
    "#communities_d stores the group of songs in the same community\n",
    "communities_d=dict.fromkeys(list(range(0,len(np.unique(list(partition.values()))))))\n",
    "for i in range(0,len(np.unique(list(partition.values())))):\n",
    "    l=[]\n",
    "    for el in partition:\n",
    "        if partition[el]==i:\n",
    "            l.append(el)\n",
    "    communities_d[i]=l\n",
    "    \n",
    "size_communities=[len(x) for x in communities_d.values()]\n",
    "\n",
    "size_communities #number of songs in each community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab215af5",
   "metadata": {},
   "source": [
    "Let's name every community for the song with highest degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7984d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_named={}\n",
    "for el in communities_d:\n",
    "    degrees=[un_Beatles_nw.degree(song) for song in communities_d[el]]\n",
    "    # pick the maximum degree and rename the community by the max degree name\n",
    "    communities_named[communities_d[el][degrees.index(max(degrees))]]=communities_d[el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a44aba8",
   "metadata": {},
   "source": [
    "#### <font color='green'> 3.4.1 _Visualize Communities:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_community = [node[1] for node in partition.items()]\n",
    "pCol = {i: list(np.random.random(size=3)) for i in set(node_community)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=2.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=0.1,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=10.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=False)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(un_Beatles_nw, pos=None, iterations=2000)\n",
    "f = dict(nx.degree(un_Beatles_nw))\n",
    "node_sizes = [v*100 for v in f.values()]\n",
    "plt.title('Communities found in Beatles network')\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "fig = plt.gcf()\n",
    "nx.draw(un_Beatles_nw, positions, node_size=node_sizes, with_labels=False, width = 0.35, node_color=list(partition.values()), alpha=0.9)\n",
    "# add legend\n",
    "legend=[]\n",
    "for idx,el in enumerate(cmap.colors):\n",
    "    legend.append(mpatches.Patch(color=cmap.colors[idx], label=list(communities_named)[idx]+' (size:'+str(size_communities[idx])+')'))\n",
    "plt.legend(handles=legend)\n",
    "# plt.savefig('Communities_beatles')\n",
    "fig.set_size_inches(20, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db413aa",
   "metadata": {},
   "source": [
    "Some first conclusions can be extracted from these communities. Although most of them share some of the most common words, some differences between them can be observed. For example in the community 4, including love me do song, love is the most used word, but ill, mine, true and cant are the other words. In contrast, in community 8, including I want to hold your hand song, although love is also the most used word, the other most common ones are say, know, good and hello. Despite this is not enough to affirm that there exist significant differences between the communities, the general perception about love seems to change, with a more sad-negative perspective in community 4 and a more optimistic-joyful love in the community 8.\n",
    "\n",
    "On the other hand, if we take a look in the unique words, in other words, in how many songs of the community a word appears, it is observed howthe most common ones are just present in half of the songs of the community, indicating that although there is a sense of belonging in each community, the partition is still far from being well done in terms of separating topics, and usually each community is a mix of different words-moods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867e1d8",
   "metadata": {},
   "source": [
    "#### <font color='green'> 3.4.2 _Analysis of the Communities:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e724e62",
   "metadata": {},
   "source": [
    "<font color='darkblue'> We extract the **most common words** in each community. \n",
    "    \n",
    "    \n",
    "- <font color='darkblue'> First, counting the number of times a word appears in each song of the community. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_text=[]\n",
    "for idx in communities_named:\n",
    "    community_words=[]\n",
    "    for el in communities_named[idx]:\n",
    "        el=el.replace(' ','-')\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        for word in clean:\n",
    "            community_words.append(word)\n",
    "    cell_text.append(FreqDist(community_words).most_common(5))\n",
    "    print(idx,': ', FreqDist(community_words).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99048bda",
   "metadata": {},
   "source": [
    "- <font color='darkblue'> Second, counting in how many songs of the community a word appear.  Below, a function is created for that purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_text=[]\n",
    "for idx in communities_d:\n",
    "    community_words=[]\n",
    "    for el in communities_d[idx]:\n",
    "        el=el.replace(' ','-')\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        unique=list(np.unique(clean))\n",
    "        for word in unique:\n",
    "            community_words.append(word)\n",
    "    cell_text.append(FreqDist(community_words).most_common(5))\n",
    "    # print('size:' ,size_communities[idx])\n",
    "    print(list(communities_named.keys())[idx], ':',FreqDist(community_words).most_common(5))\n",
    "    \n",
    "# pd.DataFrame(cell_text): just to create the tables for the markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_words(community):\n",
    "    community_words=[]\n",
    "    for el in community:\n",
    "        el=el.replace(' ','-')\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        unique=list(np.unique(clean))\n",
    "        for word in unique:\n",
    "            community_words.append(word)\n",
    "    return FreqDist(community_words).most_common(5)\n",
    "\n",
    "community_words(communities_d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c3820",
   "metadata": {},
   "source": [
    "<font color='darkblue'> We were also interested in analyzing **in which community** the most famous songs belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_community(song):\n",
    "    i=0\n",
    "    for idx in communities_d:\n",
    "        if song in communities_d[i]:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "print(list(communities_named.keys())[song_community('let it be')])\n",
    "community_words(communities_d[song_community('let it be')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d03fa0",
   "metadata": {},
   "source": [
    "|                             | Most common words (total number) |              |              |              |              | Most common unique words |            |            |             |             | Size of the community |\n",
    "|-----------------------------|:--------------------------------:|--------------|--------------|--------------|--------------|:------------------------:|------------|------------|-------------|-------------|-----------------------|\n",
    "| Comunity                    |                                  |              |              |              |              |                          |            |            |             |             |                       |\n",
    "|             0: little child |                       (come, 51) |   (know, 40) |  (honey, 32) |    (got, 31) | (things, 26) |               (know, 10) |  (love, 9) |  (like, 9) |    (say, 8) |    (got, 8) |                    21 |\n",
    "|                     1: girl |                       (girl, 93) | (better, 60) |    (let, 54) | (little, 53) |   (love, 48) |                (see, 12) | (love, 11) | (know, 11) |  (time, 11) |  (girl, 10) |                    26 |\n",
    "|                2: two of us |                        (way, 39) |   (home, 34) |    (two, 11) |    (work, 9) |    (back, 9) |                 (way, 3) |   (got, 2) |  (long, 2) | (sunday, 2) | (ticket, 2) |                     4 |\n",
    "|      3: i wanna be your man |                       (baby, 78) |   (know, 51) |    (man, 43) |   (said, 42) |   (want, 40) |               (know, 10) |  (baby, 8) |   (see, 8) |   (want, 8) |   (said, 7) |                    21 |\n",
    "|               4: love me do |                      (love, 156) |    (ill, 58) |   (mine, 46) |   (true, 27) |   (cant, 25) |               (love, 18) |  (ill, 11) | (know, 10) |    (see, 9) |   (want, 8) |                    30 |\n",
    "|         5: it won't be long |                      (yeah, 170) |   (long, 60) |   (know, 39) |    (sie, 24) |   (love, 23) |               (know, 12) | (yeah, 12) |   (day, 8) |   (long, 8) |   (well, 7) |                    22 |\n",
    "|            6: eleanor rigby |                       (know, 74) |   (name, 36) | (lonely, 28) |   (look, 27) | (number, 20) |                 (one, 5) |  (know, 5) | (going, 3) |   (time, 3) | (window, 3) |                     8 |\n",
    "|                     7: boys |                       (back, 39) |   (tell, 32) |   (goes, 26) |    (get, 22) |  (heart, 21) |                (back, 8) |  (time, 6) |  (away, 6) |    (ill, 6) |    (get, 5) |                    13 |\n",
    "| 8: i want to hold your hand |                       (love, 78) |    (say, 72) |   (know, 51) |   (good, 37) |  (hello, 35) |               (know, 17) | (love, 17) |  (say, 10) |   (like, 8) |   (time, 8) |                    29 |\n",
    "|                             |                                  |              |              |              |              |                          |            |            |             |             |                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a711d44",
   "metadata": {},
   "source": [
    "For each Commuity we aim to also analyze:\n",
    "- Total number of songs\n",
    "- Average number of words per song\n",
    "- Average number of unique words per song\n",
    "- Average number of unique words per song/total number of words per song\n",
    "- Average of number of letter per word per song (Simple or complex vocabulary)\n",
    "- Average LabMT values per song\n",
    "- Average VADER values per song\n",
    "- Average number of degrees per song\n",
    "- Most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb49d40",
   "metadata": {},
   "source": [
    "To compute most of these attributes the following function is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analysis(song):\n",
    "    clean=clean_lyrics('lyrics_songs/'+song.replace(' ','-')+'.txt')\n",
    "    # number of words per song\n",
    "    num_words=len(clean)\n",
    "    # Average number of unique words per song\n",
    "    num_unique=len(np.unique(clean))\n",
    "    # Average number of unique words per song/total number of words per song\n",
    "    percentage_unique=len(np.unique(clean))/len(clean)\n",
    "    # Average of number of letter per word per song (Simple or complex vocabulary)\n",
    "    average_length=np.mean([len(word) for word in clean])\n",
    "    \n",
    "    return num_words,num_unique,percentage_unique,average_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a0c24",
   "metadata": {},
   "source": [
    "###### <font color='green'> 3.4.2.1 _Sentimental Analysis: LabMT_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ce972",
   "metadata": {},
   "source": [
    "As aimed in the scope we want to include the Sentimental Analysis according to both LabMT and VADER criteria. In order to achieve so, below we have first defined a function that computes the average LabMT value per song. To achieve so, first the labMIT.csv is imported and the average happiness score for each word is saved in a dictionnary. Then a function is defined in which the song (input) is readed from lyrics songs folder and cleaned by applying clean_lyrics() function. Then the average score for each word of the cleaned file is stored and the mean is computed and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib auto\n",
    "labMT = pd.read_csv('labMIT.csv',sep=';')\n",
    "wordsLabMT = {}\n",
    "for word,score in zip(labMT['word'],labMT['happiness_average']):\n",
    "    wordsLabMT[word]=score\n",
    "    \n",
    "def analysis_labMT_cleaned_song(song):\n",
    "    '''To compute the sentimental analysis by using labMT we need to lemmatize, \n",
    "    set to lower case and tokenize the words so they will not used be twice. \n",
    "    In clean lyrics, this process has already been implemented.'''\n",
    "    cleaned=clean_lyrics('lyrics_songs/'+song+'.txt')\n",
    "    words=list(set(cleaned).intersection(list(wordsLabMT.keys())))\n",
    "    score=np.mean([wordsLabMT[word] for word in words])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315be00",
   "metadata": {},
   "source": [
    "###### <font color='green'> 3.6 _Sentimental Analysis: VADER_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f8b1e",
   "metadata": {},
   "source": [
    "However, sometimes maybe an analysis with VADER makes more sense, due to the mood of the songs usually resides in how the sentence is written and not just in single words. \n",
    "\n",
    "So same procedure has been done but now computing the average sentimental value of each song according to VADER criteria. Due to a lot of neutral sentences were observed, a second function has also been defined. The first one considers and computes the mean of all sentences, and therefore the results obtained is more robust. In order to observe more variation between the different songs, the second function just considers these sentences that differ from a netural value when computing the VADER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# function to compute the VADER score of a single sentence\n",
    "def sentiment_score_VADER(sentence):\n",
    "    sentiment_dict = analyzer.polarity_scores(sentence)\n",
    "    return sentiment_dict['compound']\n",
    "\n",
    "# function to compute the average VADER of a song (cleaning the lyrics)\n",
    "def analysis_VADER_cleaned_song(song):\n",
    "    song=open('lyrics_songs/'+song+'.txt').read()\n",
    "    index=[(m.start(0), m.end(0)) for m in re.finditer('  ', song)]\n",
    "    sentences=[]\n",
    "    i=0\n",
    "    for el in index:\n",
    "        sentences.append(song[i:el[0]])\n",
    "        i=el[1]\n",
    "    return np.mean([sentiment_score_VADER(el) for el in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd85890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_VADER_without_neutral_sentences(song):\n",
    "    song=open('lyrics_songs/'+song+'.txt').read()\n",
    "    index=[(m.start(0), m.end(0)) for m in re.finditer('  ', song)]\n",
    "    sentences=[]\n",
    "    i=0\n",
    "    for el in index:\n",
    "        sentences.append(song[i:el[0]])\n",
    "        i=el[1]\n",
    "    vader_values=[]\n",
    "    for el in sentences:\n",
    "        v=sentiment_score_VADER(el)\n",
    "        if v!=0:\n",
    "            vader_values.append(v)\n",
    "    if len(vader_values)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(vader_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c52aa6",
   "metadata": {},
   "source": [
    "Finally, after all the functions have been defined we proceed to create a DataFrame in order to better visualize all the results of the analysis performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36354",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df=[]\n",
    "for comm in np.unique(Lyrics_Data['Community']):\n",
    "    df=Lyrics_Data[Lyrics_Data['Community']==comm]\n",
    "    num_songs=len(df)\n",
    "    words_df=pd.DataFrame([word_analysis(song) for song in df['Song']])\n",
    "    [num_words,num_unique,percentage_unique,average_length]=np.round(np.mean(words_df),2)\n",
    "    labMT=np.mean([analysis_labMT_cleaned_song(song.replace(' ','-')) for song in df['Song']])\n",
    "    VADER=np.mean([analysis_VADER_without_neutral_sentences(song.replace(' ','-')) for song in df['Song']])\n",
    "    degree=np.mean([int(df[df['Song']==song]['degree']) for song in df['Song']])\n",
    "    comm_words=[el[0] for el in community_words(communities_d[comm])]\n",
    "    communities_df.append([num_songs,num_words,num_unique,percentage_unique,average_length,labMT,\n",
    "                           VADER,degree,comm_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities=pd.DataFrame(communities_df)\n",
    "# We rename all the different columns\n",
    "communities=communities.rename(columns={0:\"num_songs\",1:\"num_words\",2:\"num_unique\",3:\"percentage_unique\",\n",
    "                                        4:\"average_length\",5:\"average labMT\",6:\"average VADER\",\n",
    "                                        7:\"average degree\",8:\"common words\"})\n",
    "communities=communities.round(2)\n",
    "communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef478968",
   "metadata": {},
   "source": [
    "Finally, an interactive pie chart is made in order to visualizes how many degrees in total has every community, and which is the proportion with respect to the total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(Lyrics_Data_Golden_Decade2, values='degree', names='Community', title='Degrees per community')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80645057",
   "metadata": {},
   "source": [
    "To add the interactive plot in the website the following steps are required. Basically we need a username and an api key to upload the figure in chart studio. From there is where we will call the graph to be included in the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966222e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'llucoco' # your username\n",
    "api_key = 'zBqWIM1hxh393OGjtzlA' # your api key - go to profile > settings > regenerate key\n",
    "chart_studio.tools.set_credentials_file(username=username, api_key=api_key)\n",
    "# py.plot(fig, filename = 'degrees-per-community', auto_open=True)\n",
    "# pio.write_html(fig, file='index.html', auto_open=True)\n",
    "# tls.get_embed('https://plotly.com/~llucoco/3/') #change to your url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925d57b",
   "metadata": {},
   "source": [
    "## <font color='green'> 3.5 _Evolution of sentimental analysis per year_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653706f7",
   "metadata": {},
   "source": [
    "In order to have a first visualization of the results a scatter plot was aimed, where the x-axis corresponded to the year and the y-axis to the mean labMT value per song. Moreover, each community would be represented with a specific colour. In order to achieve so, a dataframe was created, including the name of each song, the year, the labMT average value and the community it belonged to. Only songs that are in the GCC and in the Beatles Golden decade (1962-1970) have been included.\n",
    "\n",
    "Below, the results obtained with the labMT analysis can be observed (note that communities 2, 5 and 8 are not represented in this graph, meaning that when filtering by Year they have been taken out of the scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_data=pd.read_csv('AllAllAtributes.csv')\n",
    "Song_data['Song']=[clean_title(title).replace('_','-') for title in Song_data['Song']]\n",
    "Lyrics_Data=pd.DataFrame({'Song':lyrics_files})\n",
    "Lyrics_Data['Year']=[Song_data[Song_data['Song']==title]['Year'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Writer']=[Song_data[Song_data['Song']==title]['Writer'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Album']=[Song_data[Song_data['Song']==title]['Album'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['LabMT']=[analysis_labMT_cleaned_song(title) for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Community']=[get_key(title.replace('-',' ')) for title in Lyrics_Data['Song']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b2eb4",
   "metadata": {},
   "source": [
    "We aim to do a scatter plot now where the x-axis will correspond to the year, and the y-axis to the mean labMT value. Also each community will be represented with a specific colour. To achieve data we need a dataframe with the name of the song, the year, the labMT value and the community it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value\n",
    "def get_key(val):\n",
    "    for key, value in communities_d.items():\n",
    "        if val in value:\n",
    "            return int(key)\n",
    "\n",
    "# if get key is None means that the song does not belong to the GCC, and therefore to any community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab0808",
   "metadata": {},
   "source": [
    "Including just the songs that are in the GCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81824b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_GCC=list(GCC.nodes)\n",
    "Song_data=pd.read_csv('AllAllAtributes.csv')\n",
    "Song_data['Song']=[clean_title(title).replace('_','-') for title in Song_data['Song']]\n",
    "Lyrics_Data=pd.DataFrame({'Song':lyrics_GCC})\n",
    "Lyrics_Data['Year']=[Song_data[Song_data['Song']==title.replace(' ','-')]['Year'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Writer']=[Song_data[Song_data['Song']==title.replace(' ','-')]['Writer'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Album']=[Song_data[Song_data['Song']==title.replace(' ','-')]['Album'].values[0] for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['LabMT']=[analysis_labMT_cleaned_song(title.replace(' ','-')) for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['Community']=[get_key(title) for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84355771",
   "metadata": {},
   "source": [
    "We will just focus on the songs realeased on Beatles golden decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyrics_Data_Golden_Decade=Lyrics_Data[Lyrics_Data['Year']<1975]\n",
    "Lyrics_Data_Golden_Decade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbcee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"LabMT\",hue=\"Community\")\n",
    "sns.lineplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"LabMT\") # show mean and 95% confidence interval\n",
    "plt.legend(title='Community')\n",
    "plt.title('Evolution of songs by year according to LabMT average values')\n",
    "# plt.savefig('LabMT evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c16c4",
   "metadata": {},
   "source": [
    "It can be seen how there is not a clear tendency in the general mood of beatles songs as years pass by. According to LabMT analysis it could be said that Beatles songs tend to slightly increase its positivy as years go by, reaching its maximun average value at year 1968. From there, their positivity slightly decreases the last two years of this golden decade before their break-up at 1970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyrics_Data['VADER']=[analysis_VADER_cleaned_song(title.replace(' ','-')) for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data['VADER_without_neutral']=[analysis_VADER_without_neutral_sentences(title.replace(' ','-')) for title in Lyrics_Data['Song']]\n",
    "Lyrics_Data_Golden_Decade=Lyrics_Data[Lyrics_Data['Year']<1975]\n",
    "Lyrics_Data_Golden_Decade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"VADER\",hue=\"Community\")\n",
    "sns.lineplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"VADER\") # show mean and 95% confidence interval\n",
    "plt.legend(title='Community')\n",
    "plt.title('Evolution of songs by year according to VADER average values')\n",
    "# plt.savefig('VADER evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"VADER_without_neutral\",hue=\"Community\")\n",
    "sns.lineplot(data=Lyrics_Data_Golden_Decade, x=\"Year\", y=\"VADER_without_neutral\") # show mean and 95% confidence interval\n",
    "plt.legend(title='Community')\n",
    "plt.title('Evolution of songs by year according to VADER average values (without considering neutral sentences)')\n",
    "plt.savefig('VADER_without_neutral_evolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6c57f",
   "metadata": {},
   "source": [
    "From the first graph, less conclusions can be extracted, just that the mood slightly decreases by year, due to it is mainly constant around an average VADER value around 0.1, which means that beatles songs tended to be more positive than negative at least. However, if a look is taken in the second graphs, some more interesting observations can be made. It still can be observed how the positivity of beatles slightly decrease during the first years of the decade, reaching an average VADER value of 0.2 between 1965-1966. From there, VADER average values slightly increases reaching a maximum around 1968-1969 before doing a significant decrease on 1970, the year of their break-up.\n",
    "Although LabMT and VADER graphs differ when showing the sentimental analysis of beatles songs by year, there a thing that can be observed in both graphs, and its this significant decrease in positivity in last year. Of course it is true that is not enough at all to affirm that, but this gives space for our imagination. Is this increase in negativity related with the fact that their break-up was just around the corner? Could the break-up of the beatles have been predicted and therefore tried to be avoided if sentimental analysis had been implemented and this decrease in positivity had been detected before it was too late?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c2fc0",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.7 _Sentimental Analysis by SongWriter:_\n",
    "\n",
    "Let's merge the different Writers in 5 categories: Harrison, Lennon, Mc Cartney, Lennon and Mc Cartney, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyrics_Data['Writer'].replace({'George Harrison': 'Harrison',\n",
    "                               'Lennon and McCartney/Nicolas/Montague': 'Lennon / McCartney',\n",
    "                               'Lennon, with McCartney':'Lennon / McCartney',\n",
    "                               'Lennon/McCartney/Harrison/Starkey': 'Lennon / McCartney',\n",
    "                               'Lennon/McCartney/Nicolas/Hellmer':'Lennon / McCartney',\n",
    "                               'McCartney, with Lennon':'Lennon / McCartney',\n",
    "                               'McCartney/Lennon':'Lennon / McCartney',\n",
    "                               'Lennon/McCartney':'Lennon / McCartney',\n",
    "                               'McCartney/Harrison':'McCartney',\n",
    "                               'Perkins/Jefferson':'Not found',\n",
    "                              }, inplace=True)\n",
    "\n",
    "for el in np.unique(Lyrics_Data['Writer']):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5ef4c",
   "metadata": {},
   "source": [
    "For each Writer we will analyze:\n",
    "- Total number of songs\n",
    "- Average number of words per song\n",
    "- Average number of unique words per song\n",
    "- Average number of unique words per song/total number of words per song\n",
    "- Average of number of letter per word per song (Simple or complex vocabulary)\n",
    "- Average LabMT values per song\n",
    "- Average VADER values per song\n",
    "- Average number of degrees per song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42588e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_df=[]\n",
    "for writer in np.unique(Lyrics_Data['Writer']):\n",
    "    print(writer)\n",
    "    df=Lyrics_Data[Lyrics_Data['Writer']==writer]\n",
    "    num_songs=len(df)\n",
    "    words_df=pd.DataFrame([word_analysis(song) for song in df['Song']])\n",
    "    [num_words,num_unique,percentage_unique,average_length]=np.round(np.mean(words_df),2)\n",
    "    labMT=np.mean([analysis_labMT_cleaned_song(song.replace(' ','-')) for song in df['Song']])\n",
    "    VADER=np.mean([analysis_VADER_without_neutral_sentences(song.replace(' ','-')) for song in df['Song']])\n",
    "    degree=np.mean([int(df[df['Song']==song]['degree']) for song in df['Song']])\n",
    "    writers_df.append([num_songs,num_words,num_unique,percentage_unique,average_length,labMT,VADER,degree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=pd.DataFrame(writers_df)\n",
    "writer=writer.rename(index={0: \"Harrison\", 1: \"Lennon\", 2: \"Lennon / McCartney\",3:\"McCartney\",4:\"Not found\"})\n",
    "writer=writer.rename(columns={0:\"num_songs\",1:\"num_words\",2:\"num_unique\",3:\"percentage_unique\",4:\"average_length\",\n",
    "                       5:\"average labMT\",6:\"average VADER\",7:\"average degree\"})\n",
    "writer=writer.round(2)\n",
    "writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7706f6d",
   "metadata": {},
   "source": [
    "From the table above, certain things can be concluded. Step by step, from the information extracted, most of the songs authorship correspond to Lennon and McCartney, which were clearly the main influences in terms of composing in the Beatles golden decade. With respect to the average number of words used per song, Harrison was the one that wrote shorter songs. However, when we compute which percentage of the total words used was unique, Harrison has the highest value. Meaning that although writing shorter songs he had slighlty a higher lexical vocabulary. We must keep in mind that this analysis is performed without considering stop words, which makes all these percentages higher. In terms of average length per word used, results show that it is almost the same in all writers, being to a small extent Lennon alone the one that wrote shorter words.\n",
    "\n",
    "The most interesting part comes when analyzing the sentimental analysis for each writer. According to both labMT and VADER, Lennon and McCartney together were the ones that wrote more postive songs. In contrast, when writing alone their songs were more pessimistic and unenthusiastic. \n",
    "\n",
    "Finally, with respect to the average degree, Lennon/McCartney have the highest value, which indicate that apart from having written a lot of songs, their topics are the most connected with others.\n",
    "\n",
    "\n",
    "In order to facilitate the comprehension of this last attribute, two interactive pie chart using plotly have been made  to compare how the proportions of degrees per writer change when compared with the number of songs per writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyrics_Data_Golden_Decade=Lyrics_Data[Lyrics_Data['Year']<1975]\n",
    "# create a subplot with 2 rows and 1 columns\n",
    "fig= px.pie(Lyrics_Data_Golden_Decade, values='degree', names='Writer', title='Degrees per writer')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# py.plot(fig, filename = 'degrees-writer', auto_open=True)\n",
    "# pio.write_html(fig, file='degreeswriter.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tls.get_embed('https://plotly.com/~llucoco/9/') #change to your url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b11652",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(writer, values='num_songs', names=writer.index, title='Number of songs per writer')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63097856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'number-songs-writer', auto_open=True)\n",
    "#pio.write_html(fig, file='songswriter.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004af003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tls.get_embed('https://plotly.com/~llucoco/11/') #change to your url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c13f3",
   "metadata": {},
   "source": [
    "From the two pie charts above, it can be concluded that although the total number of songs written per Lennon and McCartney is close to 50%, when computing the total number of degrees that come from their songs, their influence is close to 60%, meaning that their topics were the most common and related ones in all the beatles network of songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b6fb4",
   "metadata": {},
   "source": [
    "### <font color='green'> 3.8 _Analysis of the most recurrent topics over time:_\n",
    "\n",
    "Next step, has been to analyze which were the most recurrent topics over time. Due to there is a huge number of songs written is hard to extract a lot of conclusions when analizing the average LabMT and VADER per year. Thus, the next aim is to find which are the most common words used per year, and from there analyze which was the mood and the main worries this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8693a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_song(song):\n",
    "    return FreqDist(clean_lyrics('lyrics_songs/'+song.replace(' ','-')+'.txt')).most_common(5)\n",
    "\n",
    "def common_words(songs_list):\n",
    "    total_words=[]\n",
    "    for el in songs_list:\n",
    "        el=el.replace(' ','-')\n",
    "        path='lyrics_songs/'+el+'.txt'\n",
    "        clean=clean_lyrics(path)\n",
    "        # unique=list(np.unique(clean))\n",
    "        for word in clean:\n",
    "            total_words.append(word)\n",
    "    return FreqDist(total_words).most_common(5)\n",
    "\n",
    "common_words(list(Lyrics_Data[Lyrics_Data['Year']==1962]['Song']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058c17e",
   "metadata": {},
   "source": [
    "We save the 5 most common words per year in a dataframe, in order to facilitate the visualization then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[]\n",
    "for year in range(1962,1971):\n",
    "    for el in common_words(list(Lyrics_Data[Lyrics_Data['Year']==year]['Song'])):\n",
    "        d.append([el[0],year,el[1]])\n",
    "comm_words_df=pd.DataFrame(d)\n",
    "comm_words_df=comm_words_df.rename(columns={0:'Word',1: \"Year\", 2: \"Freq\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40729c",
   "metadata": {},
   "source": [
    "Now we aim to save the average labMT value of the songs that include each specific word per year, to add an extra value on the visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f284b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT_values=[]\n",
    "for year in range(1962,1971):\n",
    "    for word in list(comm_words_df[comm_words_df['Year']==year]['Word']):\n",
    "        avg_labMT=[]\n",
    "        for song in list(Lyrics_Data[Lyrics_Data['Year']==year]['Song']):\n",
    "            if word in [el[0] for el in top5_song(song)]:\n",
    "                avg_labMT.append(analysis_labMT_cleaned_song(song.replace(' ','-')))\n",
    "        labMT_values.append(np.mean(avg_labMT))\n",
    "comm_words_df['labMT value']=np.round(labMT_values,2)\n",
    "comm_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555b780",
   "metadata": {},
   "source": [
    "Plot of freq of the 5 most common words per year by showing the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613177b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'comm-words-per-year', auto_open=True)\n",
    "#pio.write_html(fig, file='commwords.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tls.get_embed('https://plotly.com/~llucoco/5/') #change to your url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(comm_words_df, x=\"Year\", y=\"Freq\", color=\"Word\",size='labMT value',size_max=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5825d",
   "metadata": {},
   "source": [
    "From the interactive graph a lot of conclusions can be extracted, referring to the sentimental analysis of beatles songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418c01e",
   "metadata": {},
   "source": [
    "#### <font color='green'> 3.8.1 _Analysis of the most common topics over time per artist:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_words_author=[]\n",
    "authors=[]\n",
    "for year in range(1962,1971):\n",
    "    df=Lyrics_Data[Lyrics_Data['Year']==year]\n",
    "    comm_words_year=[]\n",
    "    author_year=[]\n",
    "    for writer in np.unique(Lyrics_Data['Writer'])[0:4]:\n",
    "        df2=df[df['Writer']==writer]\n",
    "        try:\n",
    "            comm_words_year.append(common_words(list(df2['Song']))[0])\n",
    "            author_year.append(writer)\n",
    "        except:\n",
    "            pass\n",
    "    comm_words_author.append(comm_words_year)\n",
    "    authors.append(author_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68152c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_words_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095edaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_author_df=[]\n",
    "for word,author,year in zip(comm_words_author,authors,list(range(1962,1971))):\n",
    "    for w,a in zip(word,author):\n",
    "        word_author_df.append([w[0],w[1],a,year])\n",
    "        \n",
    "word_author_df=pd.DataFrame(word_author_df)\n",
    "word_author_df=word_author_df.rename(columns={0:'Word',1:'Freq',2:'Writer',3:'Year'})\n",
    "word_author_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(word_author_df, x=\"Year\", y=\"Freq\", color=\"Word\",symbol=\"Writer\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d93e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(fig, filename = 'comm-words-per-year-per-author', auto_open=True)\n",
    "#pio.write_html(fig, file='commwordswriter.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tls.get_embed('https://plotly.com/~llucoco/7/') #change to your url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58780b7e",
   "metadata": {},
   "source": [
    "#### <font color='green'> 3.8.2 _Analysis of the evolution of a word over time:_\n",
    "Let's do a function that creates this two plots depending on the word you want to check how it evolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lyrics_Data_Golden_Decade=Lyrics_Data[Lyrics_Data['Year']<1975]\n",
    "def word_evolution(word):\n",
    "    # we first store the number of songs that contain this word (Always considering just the Golden decade)\n",
    "    yes=0\n",
    "    songs=[]\n",
    "    for el in Lyrics_Data_Golden_Decade['Song']:\n",
    "        path='lyrics_songs/'+el.replace(' ','-')+'.txt'\n",
    "        if word in clean_lyrics(path):\n",
    "            songs.append(el.replace('-',' '))\n",
    "            yes+=1\n",
    "    print('Percentage of songs containing ', word,': ',yes/len(Lyrics_Data['Song']))\n",
    "    # we store it in a dataframe in order to do the corresponding plots\n",
    "    word_df=Lyrics_Data_Golden_Decade[Lyrics_Data_Golden_Decade['Song'].isin(songs)]\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.suptitle('Evolution of word '+str(word)+' by year')\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.scatterplot(data=word_df, x=\"Year\", y=\"LabMT\",hue=\"Writer\")\n",
    "    sns.lineplot(data=word_df, x=\"Year\", y=\"LabMT\") # show mean and 95% confidence interval\n",
    "    plt.legend(title='Author')\n",
    "    plt.title('Evolution according to LabMT average values')\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(data=word_df, x=\"Year\", y=\"VADER\",hue=\"Writer\")\n",
    "    sns.lineplot(data=word_df, x=\"Year\", y=\"VADER\") # show mean and 95% confidence interval\n",
    "    plt.legend(title='Author')\n",
    "    plt.title('Evolution according to VADER average values')\n",
    "    #plt.savefig(str(word))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "word_evolution('know')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57320a",
   "metadata": {},
   "source": [
    "In order to find interesting words let's see which are the most used words in all beatles songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_all_songs=[]\n",
    "for el in Lyrics_Data_Golden_Decade['Song']:\n",
    "    path='lyrics_songs/'+el.replace(' ','-')+'.txt'\n",
    "    for word in np.unique(clean_lyrics(path)):\n",
    "        total_words_all_songs.append(word)\n",
    "    \n",
    "FreqDist(total_words_all_songs).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91969c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_evolution('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_evolution('see')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3df107",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_evolution('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_evolution('like')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae865bbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b1953e",
   "metadata": {},
   "source": [
    "## <font color='darkgreen'> 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9bde2",
   "metadata": {},
   "source": [
    "## <font color='darkgreen'> 5. Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c48c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
